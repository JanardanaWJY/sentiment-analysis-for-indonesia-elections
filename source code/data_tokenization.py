# -*- coding: utf-8 -*-
"""Data Tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEd5U84okliUa0g_rDGfL_AptnycUN65

# **Global Needs**
"""

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# **Tokenization & Data Splitting**"""

import pandas as pd
preprocessed_paslon = "preprocessed_paslon.csv"
data = pd.read_csv(preprocessed_paslon, delimiter = ",", encoding='utf-8')

# result
print(data.head())

"""Splitting"""

from sklearn.model_selection import train_test_split

# dataset (100%) = training (70%) + temp (30%)_validation + test
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    data['tweet'], data['label'], test_size=0.3, random_state=42, shuffle=True
)

# temp (30%) = validation (50%) + test (50%)
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, random_state=42, shuffle=True
)

# Result
print(f"Training set size: {len(train_texts)}")
print(f"Validation set size: {len(val_texts)}")
print(f"Testing set size: {len(test_texts)}")

"""Tokenization"""

from transformers import BertTokenizer

"""Tokenization (IndoBert Base)/Teacher Model"""

# Load tokenizer base model
tokenizer_base = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')

# Tokenize training set
# Ensure all elements are strings
train_encodings_base = tokenizer_base([str(text) for text in train_texts], truncation=True, padding=True, max_length=512)

# Tokenize validation set
# Ensure all elements are strings
val_encodings_base = tokenizer_base([str(text) for text in val_texts], truncation=True, padding=True, max_length=512)

# Tokenize testing set
# Ensure all elements are strings
test_encodings_base = tokenizer_base([str(text) for text in test_texts], truncation=True, padding=True, max_length=512)

import json

# Saving tokenized data for base model
base_tokenized_data = {
    'train': {
        'input_ids': train_encodings_base['input_ids'],
        'attention_mask': train_encodings_base['attention_mask'],
        'labels': list(train_labels)
    },
    'val': {
        'input_ids': val_encodings_base['input_ids'],
        'attention_mask': val_encodings_base['attention_mask'],
        'labels': list(val_labels)
    },
    'test': {
        'input_ids': test_encodings_base['input_ids'],
        'attention_mask': test_encodings_base['attention_mask'],
        'labels': list(test_labels)
    }
}

# Saving the JSON file
with open('base_tokenized_data.json', 'w') as f:
    json.dump(base_tokenized_data, f)

"""Tokenization (IndoBert Lite)/Student Model"""

# Load tokenizer untuk lite model
tokenizer_lite = BertTokenizer.from_pretrained('indobenchmark/indobert-lite-base-p2')

# Tokenize training set untuk lite model
# Ensure all elements are strings by converting within a list comprehension
train_encodings_lite = tokenizer_lite([str(text) for text in train_texts], truncation=True, padding=True, max_length=512)

# Tokenize validation set untuk lite model
# Ensure all elements are strings
val_encodings_lite = tokenizer_lite([str(text) for text in val_texts], truncation=True, padding=True, max_length=512)

# Tokenize testing set untuk lite model
# Ensure all elements are strings
test_encodings_lite = tokenizer_lite([str(text) for text in test_texts], truncation=True, padding=True, max_length=512)

# Saving tokenized data for lite model
lite_tokenized_data = {
    'train': {
        'input_ids': train_encodings_lite['input_ids'],
        'attention_mask': train_encodings_lite['attention_mask'],
        'labels': list(train_labels)
    },
    'val': {
        'input_ids': val_encodings_lite['input_ids'],
        'attention_mask': val_encodings_lite['attention_mask'],
        'labels': list(val_labels)
    },
    'test': {
        'input_ids': test_encodings_lite['input_ids'],
        'attention_mask': test_encodings_lite['attention_mask'],
        'labels': list(test_labels)
    }
}

# Saving the JSON file
with open('lite_tokenized_data.json', 'w') as f:
    json.dump(lite_tokenized_data, f)