{"cells":[{"cell_type":"markdown","metadata":{"id":"ImBMfOfV-ycP"},"source":["# **Global Needs**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6QqZWJO-_yK"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-BkuhNS-4LN"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["leRate = 3e-5          # Learning rate (lebih tinggi untuk memulai optimasi lebih agresif)\n","baSize = 16            # Batch size (tetap untuk menjaga stabilitas)\n","inp_temperature = 2        # Temperature untuk Knowledge Distillation (sedikit lebih rendah untuk smoothing yang tidak terlalu agresif)\n","inp_alpha = 0.5            # Alpha (lebih fokus pada soft targets dari teacher model)"],"metadata":{"id":"QifGVQ8Rza0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q gdown\n","import gdown\n","import os\n","\n","# Pastikan folder target ada\n","os.makedirs(\"base_model_indobert_binary\", exist_ok=True)\n","\n","# File ID kamu (GANTI dengan ID asli)\n","tokenized_id = \"\"\n","config_id = \"\"\n","model_id = \"\"\n","\n","# Unduh lite_tokenized_data.json ke direktori utama\n","gdown.download(f\"https://drive.google.com/uc?id={tokenized_id}\", \"lite_tokenized_data.json\", quiet=False)\n","\n","# Unduh config.json ke dalam folder model\n","gdown.download(f\"https://drive.google.com/uc?id={config_id}\", \"base_model_indobert_binary/config.json\", quiet=False)\n","\n","# Unduh pytorch_model.bin ke dalam folder model\n","gdown.download(f\"https://drive.google.com/uc?id={model_id}\", \"base_model_indobert_binary/model.safetensors\", quiet=False, use_cookies=True)\n"],"metadata":{"id":"AB3OiPaJk8Bz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Obej3tQZIuui"},"source":["# **Knowledge Distillation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zszIFfVvIvyW"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from torch.nn import KLDivLoss, CrossEntropyLoss\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","import torch\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":659,"status":"ok","timestamp":1748150957275,"user":{"displayName":"Janardana Wijaya","userId":"08172573734881316458"},"user_tz":-420},"id":"s1vFsPJSJIR1","outputId":"0289a603-94e0-42c6-8130-61d888ab5489"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load teacher model and student model\n","teacher_model = AutoModelForSequenceClassification.from_pretrained('base_model_indobert_binary')\n","student_model = AutoModelForSequenceClassification.from_pretrained('indobenchmark/indobert-lite-base-p2', num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cFe2tppJKBa"},"outputs":[],"source":["# Freeze teacher model\n","teacher_model.eval()\n","for param in teacher_model.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaOzjVn_JSuT"},"outputs":[],"source":["# Distillation loss\n","temperature = inp_temperature\n","alpha = inp_alpha  # Balance between hard and soft labels\n","kl_loss = KLDivLoss(reduction=\"batchmean\")\n","ce_loss = CrossEntropyLoss()\n","\n","# Distillation loss function\n","def distillation_loss(student_logits, teacher_logits, labels):\n","    # Logits softmax for student and teacher\n","    soft_targets = F.log_softmax(student_logits / temperature, dim=-1)  # Use dim=-1 for flexibility\n","    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)  # Use dim=-1 for flexibility\n","\n","    # Hard target loss (CrossEntropy)\n","    hard_loss = ce_loss(student_logits, labels)\n","\n","    # Soft target loss (KLDivLoss with temperature scaling)\n","    soft_loss = kl_loss(soft_targets, teacher_probs) * (temperature ** 2)\n","\n","    # Combine losses\n","    return alpha * hard_loss + (1 - alpha) * soft_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkywZUMIJWCu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748150960409,"user_tz":-420,"elapsed":3116,"user":{"displayName":"Janardana Wijaya","userId":"08172573734881316458"}},"outputId":"3d8f5f65-fd5c-4d57-ac56-0f2e568c6a4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["import json\n","!pip install datasets\n","from datasets import Dataset\n","# load tokenized data for lite model (train and validation only)\n","with open('lite_tokenized_data.json', 'r') as f:\n","    lite_tokenized_data = json.load(f)\n","\n","train_dataset_lite = Dataset.from_dict(lite_tokenized_data['train'])\n","val_dataset_lite = Dataset.from_dict(lite_tokenized_data['val'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1748150960475,"user":{"displayName":"Janardana Wijaya","userId":"08172573734881316458"},"user_tz":-420},"id":"P3QnujpXJYwO","outputId":"20c3cefc-3a81-4c24-a84e-b8378bc2f2a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7c19981f98d0>\n","<torch.utils.data.dataloader.DataLoader object at 0x7c19aeec6bd0>\n"]}],"source":["# Collate function\n","optimizer = AdamW(student_model.parameters(), lr=leRate)\n","\n","def collate_fn(batch):\n","    return {\n","        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),\n","        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),\n","        'labels': torch.tensor([item['labels'] for item in batch], dtype=torch.long)\n","    }\n","\n","# DataLoader for train loader with collate_fn\n","train_loader = DataLoader(\n","    train_dataset_lite,\n","    batch_size=baSize,\n","    shuffle=True,\n","    collate_fn=collate_fn\n",")\n","print(train_loader)\n","\n","# DataLoader for validation dataset\n","val_loader = DataLoader(\n","    val_dataset_lite,\n","    batch_size=baSize,\n","    shuffle=False,\n","    collate_fn=collate_fn\n",")\n","print(val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KQhVUBLJbjL","outputId":"add9e595-d0fa-4ef2-d9a5-da46e58a8e18","executionInfo":{"status":"ok","timestamp":1748151697968,"user_tz":-420,"elapsed":737485,"user":{"displayName":"Janardana Wijaya","userId":"08172573734881316458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1\n","Training Loss: 0.2726\n","Validation Loss: 0.2103\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.72      0.74      0.73       278\n","    Negative       0.92      0.91      0.92       926\n","\n","    accuracy                           0.87      1204\n","   macro avg       0.82      0.83      0.82      1204\n","weighted avg       0.88      0.87      0.87      1204\n","\n","\n","Epoch 2\n","Training Loss: 0.1983\n","Validation Loss: 0.2119\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.75      0.69      0.72       278\n","    Negative       0.91      0.93      0.92       926\n","\n","    accuracy                           0.87      1204\n","   macro avg       0.83      0.81      0.82      1204\n","weighted avg       0.87      0.87      0.87      1204\n","\n","Early stopping patience counter: 1/3\n","\n","Epoch 3\n","Training Loss: 0.1665\n","Validation Loss: 0.2041\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.82      0.67      0.74       278\n","    Negative       0.91      0.96      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.86      0.81      0.83      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","\n","Epoch 4\n","Training Loss: 0.1486\n","Validation Loss: 0.2048\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.71      0.78      0.75       278\n","    Negative       0.93      0.91      0.92       926\n","\n","    accuracy                           0.88      1204\n","   macro avg       0.82      0.84      0.83      1204\n","weighted avg       0.88      0.88      0.88      1204\n","\n","Early stopping patience counter: 1/3\n","\n","Epoch 5\n","Training Loss: 0.1412\n","Validation Loss: 0.2011\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.76      0.75      0.76       278\n","    Negative       0.92      0.93      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.84      0.84      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","\n","Epoch 6\n","Training Loss: 0.1366\n","Validation Loss: 0.2046\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.82      0.66      0.73       278\n","    Negative       0.90      0.96      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.86      0.81      0.83      1204\n","weighted avg       0.88      0.89      0.88      1204\n","\n","Early stopping patience counter: 1/3\n","\n","Epoch 7\n","Training Loss: 0.1336\n","Validation Loss: 0.1982\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.75      0.77      0.76       278\n","    Negative       0.93      0.92      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.85      0.84      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","\n","Epoch 8\n","Training Loss: 0.1317\n","Validation Loss: 0.1978\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.74      0.78      0.76       278\n","    Negative       0.93      0.92      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.85      0.84      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","\n","Epoch 9\n","Training Loss: 0.1296\n","Validation Loss: 0.1958\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.75      0.79      0.77       278\n","    Negative       0.94      0.92      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.86      0.85      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","\n","Epoch 10\n","Training Loss: 0.1321\n","Validation Loss: 0.1975\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.76      0.74      0.75       278\n","    Negative       0.92      0.93      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.84      0.84      1204\n","weighted avg       0.88      0.89      0.89      1204\n","\n","Early stopping patience counter: 1/3\n","\n","Epoch 11\n","Training Loss: 0.1398\n","Validation Loss: 0.1945\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.76      0.76      0.76       278\n","    Negative       0.93      0.93      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.85      0.84      0.85      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","\n","Epoch 12\n","Training Loss: 0.1386\n","Validation Loss: 0.2100\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.77      0.72      0.74       278\n","    Negative       0.92      0.93      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.83      0.84      1204\n","weighted avg       0.88      0.89      0.88      1204\n","\n","Early stopping patience counter: 1/3\n","\n","Epoch 13\n","Training Loss: 0.1370\n","Validation Loss: 0.1994\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.72      0.80      0.76       278\n","    Negative       0.94      0.90      0.92       926\n","\n","    accuracy                           0.88      1204\n","   macro avg       0.83      0.85      0.84      1204\n","weighted avg       0.89      0.88      0.88      1204\n","\n","Early stopping patience counter: 2/3\n","\n","Epoch 14\n","Training Loss: 0.1304\n","Validation Loss: 0.1925\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.80      0.73      0.76       278\n","    Negative       0.92      0.95      0.93       926\n","\n","    accuracy                           0.90      1204\n","   macro avg       0.86      0.84      0.85      1204\n","weighted avg       0.89      0.90      0.89      1204\n","\n","\n","Epoch 15\n","Training Loss: 0.1267\n","Validation Loss: 0.1949\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.75      0.76      0.75       278\n","    Negative       0.93      0.92      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.84      0.84      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","Early stopping patience counter: 1/3\n","\n","Epoch 16\n","Training Loss: 0.1255\n","Validation Loss: 0.1952\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.75      0.78      0.77       278\n","    Negative       0.93      0.92      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.84      0.85      0.85      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","Early stopping patience counter: 2/3\n","\n","Epoch 17\n","Training Loss: 0.1247\n","Validation Loss: 0.1952\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Positive       0.76      0.77      0.77       278\n","    Negative       0.93      0.93      0.93       926\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.85      0.85      0.85      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n","Early stopping patience counter: 3/3\n","Early stopping triggered.\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","student_model.to(device)\n","teacher_model.to(device)\n","\n","best_val_loss = float('inf')\n","patience = 3\n","patience_counter = 0\n","student_model_withKD = None\n","\n","for epoch in range(50):\n","    total_train_loss = 0\n","    student_model.train()\n","\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        with torch.no_grad():\n","            teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","\n","        student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","        loss = distillation_loss(student_logits, teacher_logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        total_train_loss += loss.item()\n","\n","    # Validation\n","    total_val_loss = 0\n","    student_model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","            student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n","            loss = distillation_loss(student_logits, teacher_logits, labels)\n","            total_val_loss += loss.item()\n","\n","            preds = torch.argmax(student_logits, dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_train_loss = total_train_loss / len(train_loader)\n","    avg_val_loss = total_val_loss / len(val_loader)\n","\n","    # Output metrics\n","    print(f\"\\nEpoch {epoch + 1}\")\n","    print(f\"Training Loss: {avg_train_loss:.4f}\")\n","    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n","    print(\"Classification Report:\")\n","    print(classification_report(all_labels, all_preds, target_names=['Positive', 'Negative']))\n","\n","    # Early stopping logic\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        patience_counter = 0\n","        # Save student model\n","        student_model_withKD = student_model\n","    else:\n","        patience_counter += 1\n","        print(f\"Early stopping patience counter: {patience_counter}/{patience}\")\n","\n","        if patience_counter >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n"]},{"cell_type":"code","source":["student_model_withKD.save_pretrained('lite_model_kd')"],"metadata":{"id":"wuPfWv40t-9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***Testing/Evalution***"],"metadata":{"id":"zfC8Ys7P-DXz"}},{"cell_type":"code","source":["test_dataset_lite = Dataset.from_dict(lite_tokenized_data['test'])"],"metadata":{"id":"vRDEc8xX-CVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, dataset, collate_fn):\n","    model.to(device)\n","    model.eval()\n","    data_loader = DataLoader(dataset, batch_size=baSize, collate_fn=collate_fn)\n","    all_preds, all_labels = [], []\n","\n","    for batch in data_loader:\n","        input_ids = torch.tensor(batch['input_ids']).to(device)\n","        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","        labels = torch.tensor(batch['labels']).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            preds = torch.argmax(outputs.logits, dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return classification_report(all_labels, all_preds, target_names=['Positive', 'Negative'])"],"metadata":{"id":"I1KGR8Ou-TfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Evaluasi student model with KD (Testing Set):\")\n","print(evaluate_model(student_model_withKD, test_dataset_lite, collate_fn))"],"metadata":{"id":"ySYmEZLB-UO2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748151702867,"user_tz":-420,"elapsed":4770,"user":{"displayName":"Janardana Wijaya","userId":"08172573734881316458"}},"outputId":"c0319085-697b-4c9f-8b04-236066243abf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluasi student model with KD (Testing Set):\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-44-b4274105a7a2>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(batch['input_ids']).to(device)\n","<ipython-input-44-b4274105a7a2>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","<ipython-input-44-b4274105a7a2>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels = torch.tensor(batch['labels']).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    Positive       0.81      0.76      0.78       313\n","    Negative       0.92      0.94      0.93       891\n","\n","    accuracy                           0.89      1204\n","   macro avg       0.86      0.85      0.86      1204\n","weighted avg       0.89      0.89      0.89      1204\n","\n"]}]}],"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"13-x-qQ8g24Ux4z5GPMMnsP59V7IyQhW2","timestamp":1749724428558},{"file_id":"1Oxvd2LsPicuvLdP6cS-CRJxqiFsEMTzc","timestamp":1746436914874}],"collapsed_sections":["ImBMfOfV-ycP","Obej3tQZIuui","zfC8Ys7P-DXz"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}