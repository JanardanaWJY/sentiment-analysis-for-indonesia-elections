# -*- coding: utf-8 -*-
"""Data Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BwTaYoI0ScKlCfGXR2j8CDZIH1zlGy1U

# **Global Needs**
"""

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# **Data Preprocessing**

Dependencies and Data Preparation
"""

# Depedencies
!pip install Sastrawi

import pandas as pd
import re
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Variable for datasets
paslon = 'paslon.csv'
data = pd.read_csv(paslon, delimiter=",", encoding='ISO-8859-1')
print(data.head())

"""Preprocessing"""

# Drop unnecessary columns
columns_to_keep = ['tweet', 'label']
data = data[[col for col in columns_to_keep if col in data.columns]]

print(data.head())

# Cleaning function
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# Apply cleaning
data['tweet_cleaned'] = data['tweet'].apply(clean_text)

# Case folding
data['tweet_cleaned'] = data['tweet_cleaned'].str.lower()

# Stopword removal
stopword_factory = StopWordRemoverFactory()
stopword_remover = stopword_factory.create_stop_word_remover()
data['tweet_cleaned'] = data['tweet_cleaned'].apply(stopword_remover.remove)

# Stemming
stemmer_factory = StemmerFactory()
stemmer = stemmer_factory.create_stemmer()
data['tweet_cleaned'] = data['tweet_cleaned'].apply(stemmer.stem)

# Map labels (Positive = 0, Negative = 1)
data['label'] = data['label'].map({'Positive': 0, 'Negative': 1})

# Result
print(data.head())

# Variable for preprocessed datasets
preprocessed_paslon = 'preprocessed_paslon.csv'

# Save preprocessed data
# Replace 'tweet' column with the cleaned version
data['tweet'] = data['tweet_cleaned']
# Keep only the final 'tweet' and 'label' columns
data = data[['tweet', 'label']]
data.to_csv(preprocessed_paslon, index=False)

"""# **Data Exploration**"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Load dataset
preprocessed_paslon = "preprocessed_paslon.csv"
data = pd.read_csv(preprocessed_paslon, delimiter = ",", encoding='ISO-8859-1')

print(data.head())

# merging all tweets for vizualitation
data['tweet'] = data['tweet'].astype(str)
all_text = ' '.join(data['tweet'])

# Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Calculating class distribution
class_counts = data['label'].value_counts()  # Menghitung jumlah setiap kelas

# Label Mapping from numerical to string
class_labels = ['Negative', 'Positive']  # 0 = Positive, 1 = Negative

# Pie Chart Visual
plt.figure(figsize=(6, 6))
plt.pie(
    class_counts,
    labels=class_labels,
    autopct='%1.1f%%',
    startangle=90,
    colors=['salmon', 'skyblue']
)
plt.title("Primary Dataset")
plt.show()